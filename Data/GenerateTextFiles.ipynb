{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QH8y3EDbpAbG"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def read_file_in_chunks(file_path, chunk_size=10000):\n",
        "    chunks = []\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "        while True:\n",
        "            chunk = file.read(chunk_size).lower()\n",
        "            if not chunk:\n",
        "                break\n",
        "            chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "def split_and_save_chunks(chunks, test_ratio=0.2, train_file_path='train_file.txt', test_file_path='test_file.txt'):\n",
        "    random.shuffle(chunks)\n",
        "    split_index = int(len(chunks) * test_ratio)\n",
        "\n",
        "    test_chunks = chunks[:split_index]\n",
        "    train_chunks = chunks[split_index:]\n",
        "\n",
        "    with open(train_file_path, 'w', encoding='utf-8', errors='ignore') as train_file:\n",
        "        for chunk in train_chunks:\n",
        "            train_file.write(chunk)\n",
        "\n",
        "    with open(test_file_path, 'w', encoding='utf-8', errors='ignore') as test_file:\n",
        "        for chunk in test_chunks:\n",
        "            test_file.write(chunk)\n",
        "\n",
        "    train_length = sum(len(chunk) for chunk in train_chunks)\n",
        "    test_length = sum(len(chunk) for chunk in test_chunks)\n",
        "\n",
        "    return len(train_chunks), len(test_chunks), train_length, test_length\n",
        "\n",
        "# Example usage\n",
        "file_path = 'HarryPotter.txt'\n",
        "chunk_size = 10000  # Specify the chunk size\n",
        "chunks = read_file_in_chunks(file_path, chunk_size)\n",
        "\n",
        "total_text_length = sum(len(chunk) for chunk in chunks)\n",
        "num_chunks = len(chunks)\n",
        "\n",
        "# Split the chunks and save them to separate files\n",
        "train_chunks, test_chunks, train_length, test_length = split_and_save_chunks(chunks, test_ratio=0.05, train_file_path='train_file.txt', test_file_path='test_file.txt')\n",
        "\n",
        "print(\"Total text length:\", total_text_length)\n",
        "print(\"Total train text lenth:\", train_length)\n",
        "print(\"Total train test lenth:\", test_length)\n",
        "print('\\n')\n",
        "\n",
        "print(\"Number of chunks:\", num_chunks)\n",
        "print(\"Number of chunks in training file:\", train_chunks)\n",
        "print(\"Number of chunks in test file:\", test_chunks)\n",
        "\n"
      ]
    }
  ]
}