{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import re\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "jLqJfPWYy4oL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class TextProcessor:\n",
        "    def __init__(self, file_path):\n",
        "        self.file_path = file_path\n",
        "        self.text = self._load_file()\n",
        "\n",
        "    def _load_file(self):\n",
        "        with open(self.file_path, 'r', encoding='utf-8', errors = 'ignore') as file:\n",
        "            return file.read().lower()\n",
        "\n",
        "\n",
        "    def _clean_text(self, text):\n",
        "        text = re.sub(r\"[^a-zA-Z',.!?]\", ' ', text)\n",
        "        return text\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        len_temp = re.findall(r\"\\b\\w+(?:'\\w+)?\\b\", text)\n",
        "        print(len(len_temp))\n",
        "        tokens = re.findall(r\"\\b\\w+(?:'\\w+)?\\b|[.!,?]\", text)\n",
        "        return tokens\n",
        "    def _embed_tokens(self, tokens):\n",
        "        embedded_tokens = []\n",
        "        for token in tokens:\n",
        "            if token in self.glove_vectors:\n",
        "                embedded_tokens.append(self.glove_vectors[token])\n",
        "            else:\n",
        "                embedded_tokens.append(np.zeros(self.glove_vectors.vector_size))\n",
        "        return embedded_tokens\n",
        "\n",
        "    def process_text(self):\n",
        "        cleaned_text = self._clean_text(self.text)\n",
        "        tokens = self._tokenize(cleaned_text)\n",
        "        return tokens\n",
        "\n",
        "def _clean_text(text):\n",
        "        text = re.sub(r\"[^a-zA-Z']\", ' ', text)\n",
        "        return text\n",
        "\n",
        "def _tokenize(text):\n",
        "    tokens = re.findall(r\"\\b\\w+\\b\", text)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def process_text(text):\n",
        "    cleaned_text = _clean_text(text)\n",
        "    tokens = _tokenize(cleaned_text)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "cyGtMLb3y-YD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encoded_text, seq_length, pad_token=0):\n",
        "        self.encoded_text = encoded_text\n",
        "        self.seq_length = seq_length\n",
        "        self.pad_token = pad_token\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encoded_text) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.encoded_text[idx:idx+self.seq_length]\n",
        "        y = self.encoded_text[idx+self.seq_length]\n",
        "        return (\n",
        "            torch.tensor(x),\n",
        "            torch.tensor(y)\n",
        "        )\n",
        "\n",
        "TextProc = TextProcessor('train_file.txt')\n",
        "tokens = TextProc.process_text()\n",
        "print(\"Number of words in training text: \", len(tokens))\n",
        "token_counts = Counter(tokens)\n",
        "_w2i = {'<PAD>': 0, '<UNK>': 1}\n",
        "\n",
        "for word, count in token_counts.items():\n",
        "    _w2i[word] = len(_w2i)\n",
        "_i2w = {idx: word for word, idx in _w2i.items()}\n",
        "encoded_text = [_w2i.get(word, _w2i['<UNK>']) for word in tokens]"
      ],
      "metadata": {
        "id": "o9gWv0Ph7xvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, hidden_dim, dropout=0.1, max_len=64):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.positional_encoding = PositionalEncoding(embed_size, dropout, max_len)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "\n",
        "        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)\n",
        "        x = self.positional_encoding(x)\n",
        "        if mask == None:\n",
        "            x = self.transformer_encoder(x)\n",
        "        else:\n",
        "            x = self.transformer_encoder(x, src_key_padding_mask=mask)\n",
        "        x = self.fc_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=32):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "7bwTG4K7Ffet"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_next_k_word(model, input_text, k):\n",
        "    tokens = input_text\n",
        "    if len(tokens) > seq_length:\n",
        "        tokens = tokens[-seq_length:]\n",
        "    input_ids = torch.tensor([_w2i.get(word, _w2i['<UNK>']) for word in tokens]).unsqueeze(0).to(device)\n",
        "    outputs = model(input_ids)\n",
        "    logits = outputs[0, -1]\n",
        "    top_k_probs, top_k_indices = torch.topk(logits, k)\n",
        "    top_k_words = [_i2w[index] for index in top_k_indices.tolist()]\n",
        "    top_k_probs = top_k_probs.tolist()\n",
        "    top_k_words_probs = sorted(zip(top_k_words, top_k_probs), key=lambda x: x[1], reverse=True)\n",
        "    sorted_top_k_words = [word for word, prob in top_k_words_probs]\n",
        "\n",
        "    return sorted_top_k_words"
      ],
      "metadata": {
        "id": "WVEFBhXAyVK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_options(target_word, options, nr_of_suggestions = 5):\n",
        "        \"\"\"Target word is the next word, options is a list of the k most probable words according to the model\n",
        "        Outputs found_word which is 1 if the target_word is in options else 0\n",
        "        Outputs saved_keystrokes which is the amount of saved keystrokes, 0 if target_word not in options\"\"\"\n",
        "        if target_word in options:\n",
        "                if target_word in options[:nr_of_suggestions]:\n",
        "                    found_word = 1\n",
        "                    saved_keystrokes = len(target_word)\n",
        "                    return found_word, saved_keystrokes\n",
        "                for len_of_word in range(len(target_word)+1):\n",
        "                    options = [word for word in options if word[:len_of_word] == target_word[:len_of_word]]\n",
        "                    if target_word in options[:nr_of_suggestions]:\n",
        "                        found_word = 1\n",
        "                        saved_keystrokes = len(target_word) - len_of_word\n",
        "                        return found_word, saved_keystrokes\n",
        "                return 0, 0\n",
        "        else:\n",
        "            found_word = 0\n",
        "            saved_keystrokes = 0\n",
        "            return found_word, saved_keystrokes\n",
        "\n",
        "\n",
        "def evaluate_text(model, tokens, neighbors=100):\n",
        "        saved_keystrokes = 0\n",
        "        total_keystrokes = 0\n",
        "        model.eval()\n",
        "        found_words = 0\n",
        "        total_words = 0\n",
        "        for index, word in enumerate(tokens[1:],1):\n",
        "            if index < seq_length:\n",
        "                input_indices = tokens[:index]\n",
        "            else:\n",
        "                input_indices = tokens[index-seq_length :index]\n",
        "            top_words = get_next_k_word(model, input_indices, neighbors)\n",
        "            f_w, s_k = evaluate_options(word, top_words, nr_of_suggestions = 5)\n",
        "            saved_keystrokes += s_k\n",
        "            found_words += f_w\n",
        "            total_words += 1\n",
        "            total_keystrokes += len(word)\n",
        "\n",
        "            input_indices.append(index)\n",
        "\n",
        "        print(f'Saved keystroke percentage: {100*saved_keystrokes/total_keystrokes:.2f}%')\n",
        "        print(f'Found words percentage: {100*found_words/total_words:.2f}%')\n"
      ],
      "metadata": {
        "id": "9ptBwQA27SfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train(model, nr_of_epochs, lr, dataloader):\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_curve = []\n",
        "    for epoch in range(nr_of_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for inputs, targets in tqdm(dataloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            outputs = outputs[:, -1, :]\n",
        "            loss = loss_func(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}')\n",
        "        loss_curve.append(total_loss / len(dataloader))\n",
        "\n",
        "    return loss_curve\n",
        "\n",
        "seq_length = 7\n",
        "traindataset = TextDataset(encoded_text, seq_length)\n",
        "traindataloader = DataLoader(traindataset, batch_size=32, shuffle=True)\n",
        "vocab_size = len(_w2i)\n",
        "embed_size = 256\n",
        "num_heads = 8\n",
        "multiplier = 4\n",
        "num_layers = 4\n",
        "hidden_dim = embed_size * multiplier\n",
        "lr = 1e-4\n",
        "val_loss = []\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "TextProc = TextProcessor('test_file.txt')\n",
        "tokens = TextProc.process_text()\n",
        "temp = []\n",
        "model = TransformerModel(vocab_size, embed_size, num_heads, num_layers, hidden_dim).to(device)\n",
        "val_loss = train(model, nr_of_epochs = 4, lr = lr, dataloader = traindataloader)\n",
        "evaluate_text(model, tokens, 100)\n"
      ],
      "metadata": {
        "id": "BOK3Jul0exTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'w2i': _w2i,\n",
        "            'i2w': _i2w,\n",
        "            'embedding_dim': embed_size,\n",
        "            'hidden_size': hidden_dim,\n",
        "            'drop_out': 0.1,\n",
        "            'num_layers': num_layers,\n",
        "            'num_heads': num_heads,\n",
        "            'vocab_size': vocab_size,\n",
        "            'sequence_length': seq_length,\n",
        "            'eval_text': tokens\n",
        "        }, \"model.pth\")"
      ],
      "metadata": {
        "id": "oc0ojFPHOWku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_model(file_path):\n",
        "    checkpoint = torch.load(file_path)\n",
        "    _w2i = checkpoint['w2i']\n",
        "    _i2w = checkpoint['i2w']\n",
        "    embed_size = checkpoint['embedding_dim']\n",
        "    hidden_dim = checkpoint['hidden_size']\n",
        "    num_layer = checkpoint['num_layers']\n",
        "    vocab_size = checkpoint['vocab_size']\n",
        "    drop_out = checkpoint['drop_out']\n",
        "    num_heads = checkpoint['num_heads']\n",
        "    model = TransformerModel(vocab_size, embed_size, num_heads, num_layer, hidden_dim).to(device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    return model, _w2i, _i2w\n",
        "\n",
        "seq_length = 7\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model, _w2i, _i2w =load_model(\"model.pth\")\n",
        "\n",
        "TextProc = TextProcessor('test_file.txt')\n",
        "tokens = TextProc.process_text()\n",
        "\n",
        "evaluate_text(model, tokens, 100)\n"
      ],
      "metadata": {
        "id": "eoYfPi-5SDx6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}