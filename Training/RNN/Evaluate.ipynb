{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMuQbc5PvqrKy1jWeKCjo/b"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","import re\n","import numpy as np\n","import time\n","\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import time\n","from collections import deque\n","\n","from sklearn.model_selection import GridSearchCV\n","import random\n"],"metadata":{"id":"2uNe43Ti4fNd","executionInfo":{"status":"ok","timestamp":1716734338202,"user_tz":-120,"elapsed":7889,"user":{"displayName":"Emilia Sjögren","userId":"10339343878085003295"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"QH8y3EDbpAbG","colab":{"base_uri":"https://localhost:8080/","height":412},"executionInfo":{"status":"error","timestamp":1716734338206,"user_tz":-120,"elapsed":25,"user":{"displayName":"Emilia Sjögren","userId":"10339343878085003295"}},"outputId":"57dc587b-40d7-40c2-812f-8a5ad8f5056c"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'HarryPotter.txt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-f73101dabe06>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'HarryPotter.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mchunk_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m  \u001b[0;31m# Specify the chunk size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file_in_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mtotal_text_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-f73101dabe06>\u001b[0m in \u001b[0;36mread_file_in_chunks\u001b[0;34m(file_path, chunk_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_file_in_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'HarryPotter.txt'"]}],"source":["import random\n","\n","def read_file_in_chunks(file_path, chunk_size=10000):\n","    chunks = []\n","    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n","        while True:\n","            chunk = file.read(chunk_size).lower()\n","            if not chunk:\n","                break\n","            chunks.append(chunk)\n","    return chunks\n","\n","def split_and_save_chunks(chunks, test_ratio=0.2, train_file_path='train_file.txt', test_file_path='test_file.txt'):\n","    random.shuffle(chunks)\n","    split_index = int(len(chunks) * test_ratio)\n","\n","    test_chunks = chunks[:split_index]\n","    train_chunks = chunks[split_index:]\n","\n","    with open(train_file_path, 'w', encoding='utf-8', errors='ignore') as train_file:\n","        for chunk in train_chunks:\n","            train_file.write(chunk)\n","\n","    with open(test_file_path, 'w', encoding='utf-8', errors='ignore') as test_file:\n","        for chunk in test_chunks:\n","            test_file.write(chunk)\n","\n","    train_length = sum(len(chunk) for chunk in train_chunks)\n","    test_length = sum(len(chunk) for chunk in test_chunks)\n","\n","    return len(train_chunks), len(test_chunks), train_length, test_length\n","\n","# Example usage\n","file_path = 'HarryPotter.txt'\n","chunk_size = 10000  # Specify the chunk size\n","chunks = read_file_in_chunks(file_path, chunk_size)\n","\n","total_text_length = sum(len(chunk) for chunk in chunks)\n","num_chunks = len(chunks)\n","\n","# Split the chunks and save them to separate files\n","train_chunks, test_chunks, train_length, test_length = split_and_save_chunks(chunks, test_ratio=0.05, train_file_path='train_file1.txt', test_file_path='test_file1.txt')\n","\n","print(\"Total text length:\", total_text_length)\n","print(\"Total train text lenth:\", train_length)\n","print(\"Total train test lenth:\", test_length)\n","print('\\n')\n","\n","print(\"Number of chunks:\", num_chunks)\n","print(\"Number of chunks in training file:\", train_chunks)\n","print(\"Number of chunks in test file:\", test_chunks)\n","\n"]},{"cell_type":"code","source":["\n","class TextProcessor:\n","    def __init__(self, file_path, sequence_length):\n","        self.vocab = []\n","        self._i2w = {}\n","        self._w2i = {}\n","\n","        self.file_path = file_path\n","        self.unprocessed_text = self._load_file(self.file_path)\n","        self.text = self.process_text(self.unprocessed_text)\n","        self.create_vocab(self.text)\n","\n","        self.sequence_length = sequence_length\n","        self._vocablength = len(self.vocab)\n","        print(self._vocablength)\n","\n","    def _load_file(self, file_path):\n","        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n","            return file.read().lower()\n","\n","    def _clean_text(self, text):\n","        text = re.sub(r\"[^a-zA-Z',.!]\", ' ', text)\n","        return text\n","\n","    def _tokenize(self, text):\n","        tokens = re.findall(r\"\\b\\w+(?:'\\w+)?\\b|[.!,?]\", text)\n","        return tokens\n","\n","    def process_text(self, text):\n","        cleaned_text = self._clean_text(text)\n","        tokens = self._tokenize(cleaned_text)\n","        return tokens\n","\n","    def create_vocab(self, text):\n","        # Add an unknown word\n","        self.vocab.append('<UNK>')\n","        self._i2w[0] = '<UNK>'\n","        self._w2i['<UNK>'] = 0\n","\n","        idx = 1\n","        for i, word in enumerate(text):\n","            if word not in self.vocab:\n","                self.vocab.append(word)\n","                self._i2w[idx] = word\n","                self._w2i[word] = idx\n","                idx += 1\n","\n","    def text_to_indices(self, text):\n","        indices = []\n","        for word in text:\n","            if word in self._w2i:\n","                indices.append(self._w2i[word])\n","            else:\n","                indices.append(0)\n","        return indices\n","\n","    def generate_sequences(self):\n","        sequences = []\n","        for i in range(0, len(self.text) - self.sequence_length):\n","            sequences.append(self.text_to_indices(self.text[i:i + self.sequence_length]))\n","        return np.array(sequences)\n","\n","    def generate_input_target_sequences(self):\n","        sequences = self.generate_sequences()\n","        inputs = sequences[:, :-1]  # Exclude the last word from inputs\n","        targets = sequences[:, -1]   # Exclude the first word from targets\n","        return inputs, targets\n","\n","    def generate_dataloader(self, batch_size):\n","        inputs, targets = self.generate_input_target_sequences()\n","\n","        # Convert numpy arrays to PyTorch tensors\n","        inputs = torch.LongTensor(inputs)\n","        targets = torch.LongTensor(targets)\n","\n","        # Create DataLoader for batching and shuffling data\n","        dataset = TensorDataset(inputs, targets)\n","        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","        return train_loader\n","\n","\n"],"metadata":{"id":"FCyJ66VN4PBj","executionInfo":{"status":"ok","timestamp":1716734994899,"user_tz":-120,"elapsed":470,"user":{"displayName":"Emilia Sjögren","userId":"10339343878085003295"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["tp = TextProcessor('test_file.txt', 8)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vax_ehhd4XUE","executionInfo":{"status":"ok","timestamp":1716735012762,"user_tz":-120,"elapsed":1265,"user":{"displayName":"Emilia Sjögren","userId":"10339343878085003295"}},"outputId":"c93e7d35-be77-451c-fd15-b222149e68c5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["6012\n"]}]},{"cell_type":"code","source":["len(tp.text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XgD-KdFz4q5n","executionInfo":{"status":"ok","timestamp":1716677135638,"user_tz":-120,"elapsed":8,"user":{"displayName":"Emilia Sjögren","userId":"10339343878085003295"}},"outputId":"61386a60-22a1-4225-a83d-23b489ad4220"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["62983"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","import re\n","import numpy as np\n","import time\n","\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import time\n","from collections import deque\n","\n","from sklearn.model_selection import GridSearchCV\n","import random\n"],"metadata":{"id":"4JLLnTJLY_4S","executionInfo":{"status":"ok","timestamp":1716735403365,"user_tz":-120,"elapsed":5313,"user":{"displayName":"Emilia Sjögren","userId":"10339343878085003295"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["\n","class DeepNetwork(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim=100, hidden_size=256, num_layers=2, drop_out=0.0):\n","        super(DeepNetwork, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.rnn = nn.GRU(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True, dropout=drop_out)\n","        self.final = nn.Linear(hidden_size, vocab_size)\n","\n","    def forward(self, t, hidden):\n","        embeddings = self.embedding(t)\n","        out, hidden = self.rnn(embeddings, hidden)\n","        out = self.final(out[:, -1, :])\n","        return out, hidden\n","\n"],"metadata":{"id":"beYcCJOOZBuM","executionInfo":{"status":"ok","timestamp":1716735404960,"user_tz":-120,"elapsed":5,"user":{"displayName":"Emilia Sjögren","userId":"10339343878085003295"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class Evaluate:\n","    def __init__(self, file_path, device='cpu'):\n","        self.device = device\n","        self.load_model(file_path)\n","\n","    def load_model(self, file_path):\n","        checkpoint = torch.load(file_path, map_location=self.device)\n","\n","        # Reconstruct the text processor\n","        self._w2i = checkpoint['w2i']\n","        self._i2w = checkpoint['i2w']\n","\n","        # Retrieve model parameters\n","        embedding_dim = checkpoint['embedding_dim']\n","        hidden_size = checkpoint['hidden_size']\n","        num_layers = checkpoint['num_layers']\n","        vocab_size = checkpoint['vocab_size']\n","        drop_out = checkpoint['drop_out']\n","\n","        # Reconstruct the model with parameters loaded from the checkpoint\n","        self.model = DeepNetwork(vocab_size, embedding_dim, hidden_size, num_layers=num_layers).to(self.device)\n","        self.model.load_state_dict(checkpoint['model_state_dict'])\n","        self.model.eval()\n","\n","        # Load losses and accuracies\n","        self.losses = checkpoint['losses']\n","        self.accuracies = checkpoint['accuracies']\n","        self.accuracies_words = checkpoint['accuracies_words']\n","        self.epoch_acc = checkpoint['epoch_acc']\n","\n","        self.sequence_length = checkpoint['sequence_length']\n","        self.num_layers = checkpoint['num_layers']\n","        self.hidden_size = checkpoint['hidden_size']\n","        self.eval_text = checkpoint['eval_text']\n","\n","        print(f\"Model loaded from {file_path}\")\n","\n","    def generate_text(self, seed_text, next_words=50):\n","        self.model.eval()\n","        words = seed_text.lower().split()\n","        hidden = None\n","\n","        for _ in range(next_words):\n","            input_indices = [self._w2i[word] if word in self._w2i else 0 for word in words[-self.sequence_length:]]\n","            input_tensor = torch.LongTensor([input_indices]).to(self.device)\n","\n","            with torch.no_grad():\n","                output = self.model(input_tensor)\n","\n","            output = output.squeeze(0)[-1]\n","            probabilities = F.softmax(output, dim=0).cpu().numpy()\n","\n","            top_indices = np.argsort(probabilities)[-5:][::-1]\n","            top_words = [self._i2w[idx] for idx in top_indices]\n","            top_probs = probabilities[top_indices]\n","\n","            predicted_index = np.random.choice(top_indices, p=top_probs/top_probs.sum())\n","            predicted_word = self._i2w[predicted_index]\n","            words.append(predicted_word)\n","\n","        return ' '.join(words)\n","\n","    def _generate_words(self, input_indices, neighbours, hidden):\n","        input_tensor = torch.LongTensor([input_indices]).to(self.device)\n","        with torch.no_grad():\n","            output, hidden = self.model(input_tensor, hidden)\n","\n","        output = output.squeeze(0)\n","\n","        probabilities = F.softmax(output, dim=0).cpu().numpy()\n","\n","        top_indices = np.argsort(probabilities)[-neighbours:][::-1]\n","        top_words = [self._i2w[idx] for idx in top_indices]\n","        top_probs = probabilities[top_indices]\n","\n","        return top_indices, top_words, hidden\n","\n","\n","    def evaluate_text(self, neighbours=100):\n","        start = self.eval_text[0]\n","        saved_keystrokes = 0\n","        total_keystrokes = 0\n","\n","        found_words = 0\n","        total_words = 0\n","\n","        input_indices = deque([start], maxlen=self.sequence_length-1)\n","\n","        for index in self.eval_text[1:]:\n","            hidden = torch.zeros(self.num_layers, 1, self.hidden_size).to(self.device)\n","            top_indices, top_words, hidden = self._generate_words(input_indices, neighbours, hidden)\n","\n","            fw, sk = self.evaluate_options(self._i2w[index], top_words)\n","\n","            saved_keystrokes += sk\n","            found_words += fw\n","\n","            total_words += 1\n","            total_keystrokes += len(self._i2w[index])  # Adjust index to get the actual item\n","            input_indices.append(index)  # Adjust index to get the actual item\n","\n","        print(f'Saved keystroke percentage: {100*saved_keystrokes/total_keystrokes:.2f}%')\n","        print(f'Found words percentage: {100*found_words/total_words:.2f}%')\n","        return saved_keystrokes/total_keystrokes, found_words/total_words\n","\n","\n","    def evaluate_options(self, target_word, options, nr_of_suggestions=5):\n","        \"\"\"Target word is the next word, options is a list of the k most probable words according to the model\n","        Outputs found_word which is 1 if the target_word is in options else 0\n","        Outputs saved_keystrokes which is the amount of saved keystrokes, 0 if target_word not in options\"\"\"\n","        if target_word in options:\n","            if target_word in options[:nr_of_suggestions]:\n","                found_word = 1\n","                saved_keystrokes = len(target_word)\n","                return found_word, saved_keystrokes\n","            for len_of_word in range(len(target_word)+1):\n","                options = [word for word in options if word[:len_of_word] == target_word[:len_of_word]]\n","                if target_word in options[:nr_of_suggestions]:\n","                    found_word = 1\n","                    saved_keystrokes = len(target_word) - len_of_word\n","                    return found_word, saved_keystrokes\n","            return (0, 0)\n","\n","        else:\n","            found_word = 0\n","            saved_keystrokes = 0\n","            return found_word, saved_keystrokes\n","\n","\n"],"metadata":{"id":"IiNUYhXtZDoc","executionInfo":{"status":"ok","timestamp":1716736478903,"user_tz":-120,"elapsed":4,"user":{"displayName":"Emilia Sjögren","userId":"10339343878085003295"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TyxlFIaxZvxg","executionInfo":{"status":"ok","timestamp":1716736480847,"user_tz":-120,"elapsed":273,"user":{"displayName":"Emilia Sjögren","userId":"10339343878085003295"}},"outputId":"56ef04ac-7545-414f-c978-0ed2cb3776c8"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","source":["evale = Evaluate('model.pth', device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0nXIKnt0ZF67","executionInfo":{"status":"ok","timestamp":1716736482935,"user_tz":-120,"elapsed":7,"user":{"displayName":"Emilia Sjögren","userId":"10339343878085003295"}},"outputId":"a3af18e4-b6bd-43e8-f72a-6465c48d7ebb"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Model loaded from model.pth\n"]}]},{"cell_type":"code","source":["evale.evaluate_text(neighbours=5)\n","\n","evale.evaluate_text(neighbours=20)\n","\n","evale.evaluate_text(neighbours=50)\n","\n","evale.evaluate_text(neighbours=100)\n","\n","evale.evaluate_text(neighbours=200)\n","\n","evale.evaluate_text(neighbours=500)\n","\n","evale.evaluate_text(neighbours=1000)\n","\n","evale.evaluate_text(neighbours=2000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5tRoB841ZIQU","executionInfo":{"status":"ok","timestamp":1716738272739,"user_tz":-120,"elapsed":1396391,"user":{"displayName":"Emilia Sjögren","userId":"10339343878085003295"}},"outputId":"2159fa73-2edd-4c4e-cfde-93bd8be5aa8b"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved keystroke percentage: 28.81%\n","Found words percentage: 42.47%\n","Saved keystroke percentage: 40.83%\n","Found words percentage: 59.67%\n","Saved keystroke percentage: 49.32%\n","Found words percentage: 70.12%\n","Saved keystroke percentage: 55.16%\n","Found words percentage: 76.95%\n","Saved keystroke percentage: 60.04%\n","Found words percentage: 82.50%\n","Saved keystroke percentage: 65.22%\n","Found words percentage: 88.32%\n","Saved keystroke percentage: 68.10%\n","Found words percentage: 91.58%\n","Saved keystroke percentage: 70.27%\n","Found words percentage: 94.16%\n"]},{"output_type":"execute_result","data":{"text/plain":["(0.7027321874469684, 0.9415547299228351)"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["evale = Evaluate('model.pth', device)\n","evale.evaluate_text(neighbours=100)"],"metadata":{"id":"AYCdyhEic3wP"},"execution_count":null,"outputs":[]}]}